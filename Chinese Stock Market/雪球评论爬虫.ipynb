{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 雪球评论爬虫\n",
    "\n",
    "Alex\n",
    "\n",
    "Created on: 08/07/2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json                     # 爬取结果为JSON格式，用这个进行格式化\n",
    "import os                       # 转换工作路径\n",
    "import sys                      # 获取当前python文件所在路径\n",
    "import time                     # 计时\n",
    "\n",
    "import pandas as pd\n",
    "import requests                 # 也可以选用selenium，但是requests代码好写易看一点\n",
    "from bs4 import BeautifulSoup   # 用于格式化网页CSS结构\n",
    "\n",
    "key_word        =   '伊利股份'      # key_word 是要查询的内容，可以是股票名，股票代码，行业名称\n",
    "comments_num    =   1000          # 要爬去的评论数。最新的N条\n",
    "\n",
    "# 爬取的结果是JSON格式转为Dict，需要进行二次转换\n",
    "text_keys       =   ['title','view_count','like_count']\n",
    "user_info_keys  =   ['description','screen_name',\n",
    "                    'followers_count','friends_count','gender','province']\n",
    "df_columns      =   ['text','created_at'] + text_keys + user_info_keys\n",
    "\n",
    "os.chdir(sys.path[0])   # 更改工作路径，方便输出结果\n",
    "tic=time.time()         # 计时\n",
    "\n",
    "\n",
    "page_num=comments_num//10+1     # 雪球每页显示十个评论\n",
    "print('%s comments to crawl, script started!'%comments_num)\n",
    "\n",
    "s=requests.session()            # 爬虫主体对象\n",
    "# 雪球反爬虫通过监察访问单位的headers来进行筛查，需要加上User-Agent和Cookie\n",
    "print('Original header')\n",
    "print(s.headers) # 运行这个可以看到没有更改headers的时候，User-Agent是python\n",
    "\n",
    "s.headers.update({'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'})\n",
    "## 改成自己的cookie\n",
    "s.headers.update({'Cookie': 'YOUR_COOKIE'})\n",
    "# print(s.headers) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000 comments to crawl, script started!\n",
      "Original header\n",
      "{'User-Agent': 'python-requests/2.24.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_one_page(key_word,page_index):\n",
    "    \"\"\"\n",
    "    获取单一页面评论\n",
    "    \"\"\"\n",
    "    # 雪球评论页面采用动态加载，分析xhr文件发现，每次加载均访问这个query\n",
    "    # 返回包含下十个comments的json文件，这个url主要部分是q=搜索内容以及page=页码\n",
    "    # 更改count内容并不能增加每页显示的评论数\n",
    "    url='https://xueqiu.com/query/v1/search/status.json?sortId=1&q=%s&count=20&page=%s'%(key_word,page_index)\n",
    "    for iii in range(10):\n",
    "        try:\n",
    "            r=s.get(url).text           # 调用爬虫主体对象访问目标url并返回网站text\n",
    "            res_temp=json.loads(r)      # 这个text结构式json，格式化为dict方便处理\n",
    "            res_temp=res_temp['list']   # 观察发现只有list中的内容是我们需要的\n",
    "            break\n",
    "        except:\n",
    "            # 有的时候会遇到反爬虫现象，简单的访问过于频繁以及次数过多，歇几秒就行了\n",
    "            print('comments #%s failed x%s, sleep for 3s, timer: %ss'%(len(res_raw),iii,round(time.time()-tic,2)))\n",
    "            time.sleep(3)\n",
    "    \n",
    "    if page_index%5==0: \n",
    "        # 每爬5页print一下进度\n",
    "        print('comments got %s/%s, timer: %ss'%(len(res_raw),comments_num,round(time.time()-tic,2)))\n",
    "    time.sleep(0.3) # 每次访问都歇一歇，免得被block\n",
    "\n",
    "    return res_temp\n",
    "\n",
    "def single_processor(dct):\n",
    "    \"\"\"\n",
    "    每个comment的结果仍为json（或dict）我们需要格式化一下，转换成Excel方便分析\n",
    "    \"\"\"\n",
    "    res=[]                              # 空list，用于存储处理结果\n",
    "\n",
    "    text=dct['text']                    # 获取text键，这是评论的内容\n",
    "    text=BeautifulSoup(text)            # 这个内容是html格式，需要用beautiful soup进行格式化\n",
    "    \n",
    "    text=text.find_all('body')[0]       # body 标签下的是我们要的东西\n",
    "    text=text.get_text()                # 获取该结构下的文本\n",
    "    text=text.replace('\\xa0','')        # 有一些错误代码，删除\n",
    "    res.append(text)                    \n",
    "\n",
    "    # json套json，这部分处理获取comments的发表时间，有的评论没有这一些，记为none\n",
    "    try:    res.append(eval(dct['trackJson'])['created_at'])\n",
    "    except: res.append('none')\n",
    "\n",
    "    # 将comments标题，浏览数，点赞数提取\n",
    "    for key in text_keys:\n",
    "        res.append(dct[key])\n",
    "\n",
    "    # 提取作者信息，简介，用户名等\n",
    "    user=dct['user']\n",
    "    for key in user_info_keys:\n",
    "        try:res.append(user[key])\n",
    "        except:res.append('none')\n",
    "    return res\n",
    "\n",
    "\n",
    "res_raw=[]\n",
    "for i in range(page_num):   # 依次访问每一页\n",
    "    res_raw.extend(get_one_page(key_word,i))\n",
    "print(len(res_raw),'comments got!')\n",
    "\n",
    "\n",
    "res=list(map(lambda x: single_processor(x),res_raw))    # 分布式处理每一个comments\n",
    "res=pd.DataFrame(res,columns=df_columns)                # 将结果存储在dataframe\n",
    "res.rename(columns={'description':'user_description',   # 列重命名\n",
    "                    'screen_name':'user_name'},\n",
    "           inplace=True)\n",
    "\n",
    "res"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "comments got 0/1000, timer: 4.96s\n",
      "comments got 50/1000, timer: 9.97s\n",
      "comments got 100/1000, timer: 16.16s\n",
      "comments got 148/1000, timer: 22.46s\n",
      "comments got 198/1000, timer: 27.17s\n",
      "comments got 248/1000, timer: 31.98s\n",
      "comments got 298/1000, timer: 39.58s\n",
      "comments got 347/1000, timer: 47.27s\n",
      "comments got 397/1000, timer: 54.81s\n",
      "comments got 447/1000, timer: 59.22s\n",
      "comments got 496/1000, timer: 66.52s\n",
      "comments got 546/1000, timer: 73.35s\n",
      "comments got 596/1000, timer: 81.13s\n",
      "comments #626 failed x0, sleep for 3s, timer: 85.12s\n",
      "comments got 646/1000, timer: 90.96s\n",
      "comments got 695/1000, timer: 99.05s\n",
      "comments got 745/1000, timer: 105.84s\n",
      "comments got 795/1000, timer: 112.57s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78c62bd594015071fc94ec09f8f0c8693d5df646d29fd961f83262230d581249"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.998419,
   "end_time": "2021-05-19T01:48:38.046799",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-19T01:48:29.048380",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}